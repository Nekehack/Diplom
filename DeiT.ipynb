{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c896cd-f84a-4cde-bc54-156a0ee93719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class HardDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Работает с любым количеством классов\n",
    "        \n",
    "    def forward(self, inputs: Tensor, outputs: tuple[Tensor, Tensor], labels: Tensor) -> Tensor:\n",
    "        outputs_cls, outputs_dist = outputs\n",
    "        \n",
    "        # Базовая потеря (CLS)\n",
    "        base_loss = self.criterion(outputs_cls, labels)\n",
    "\n",
    "        # Вычисляем предсказания учителя\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(inputs)\n",
    "        teacher_labels = torch.argmax(teacher_outputs, dim=1)  # Теперь метки 0, 1, 2\n",
    "\n",
    "        # Потеря для DIST\n",
    "        teacher_loss = self.criterion(outputs_dist, teacher_labels)\n",
    "\n",
    "        return 0.5 * base_loss + 0.5 * teacher_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c80bc0c-daa3-4142-a3d7-7801cddf5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "059aed93-1f3e-45b7-bf00-5b13fe5b884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Проекция патчей\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "        # Токены CLS и DIST\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, emb_size))  # Убедитесь, что это определено\n",
    "\n",
    "        # Позиционные эмбеддинги\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positions = nn.Parameter(torch.randn(num_patches + 2, emb_size))  # +2 для cls_token и dist_token\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "\n",
    "        # Проекция патчей\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Создание токенов CLS и DIST\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        dist_tokens = repeat(self.dist_token, '() n e -> b n e', b=b)\n",
    "\n",
    "        # Добавление токенов CLS и DIST к входным данным\n",
    "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)\n",
    "\n",
    "        # Добавление позиционных эмбеддингов\n",
    "        x += self.positions\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7500a9-ead7-47af-9e25-7f74871b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 2):       \n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Linear(emb_size, n_classes)\n",
    "        self.dist_head = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, x_dist = x[:, 0], x[:, 1]\n",
    "        x_head = self.head(x)\n",
    "        x_dist_head = self.dist_head(x_dist)\n",
    "        \n",
    "        if self.training:\n",
    "            x = x_head, x_dist_head  # Возвращает кортеж\n",
    "        else:\n",
    "            x = (x_head + x_dist_head) / 2  # Возвращает тензор\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f568d11d-94c0-4ce3-a637-81880310bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ec11eb-def9-44d0-99d4-e4eb6c10f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7881cdbb-bd37-45fb-8b0f-86bf71790754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anomaly', 'Brain_tumor', 'Health']\n",
      "822\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Определение преобразований для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Изменяем размер до 224x224\n",
    "    transforms.ToTensor(),          # Преобразуем в тензор\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация\n",
    "])\n",
    "\n",
    "# Создание датасета с помощью ImageFolder\n",
    "ds = datasets.ImageFolder(root='TestingAndTrainingFinal_min', transform=transform)\n",
    "\n",
    "# Создание DataLoader\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=False)\n",
    "\n",
    "print(ds.classes)  # ['Anomaly', 'Brain_tumor', 'Health', ]\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a047bb-4d56-4803-838c-459ac4bdead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 3])\n",
      "Выход учителя: torch.Size([1, 3])\n",
      "Выход студента: tensor([[-0.1888,  0.2712,  0.4434]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████| 26/26 [05:33<00:00, 12.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████| 26/26 [07:13<00:00, 16.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████| 26/26 [06:51<00:00, 15.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████| 26/26 [05:54<00:00, 13.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████| 26/26 [06:29<00:00, 14.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████| 26/26 [05:49<00:00, 13.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████| 26/26 [05:34<00:00, 12.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.6236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████████| 26/26 [05:21<00:00, 12.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████████| 26/26 [05:17<00:00, 12.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████████| 26/26 [05:10<00:00, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6711\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam #\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Teacher model (Vision Transformer)\n",
    "teacher = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=3)\n",
    "teacher.eval()\n",
    "\n",
    "# Student model (DeiT)\n",
    "student = timm.create_model('deit_small_patch16_224', pretrained=True, num_classes=3)\n",
    "\n",
    "# teacher = ViT.vit_large_patch16_224()\n",
    "# student = DeiT.deit_small_patch16_224()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Создание датасета\n",
    "\n",
    "# ds = datasets.ImageFolder(\n",
    "#     root='archive-4',\n",
    "#     transform=transform,\n",
    "#     target_transform=lambda x: 0 if ds.classes[x] == 'tumor' else 1\n",
    "# )\n",
    "ds = datasets.ImageFolder(root='TestingAndTrainingFinal_min', transform=transform)\n",
    "\n",
    "# Создание DataLoader\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "teacher.to(device)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "output = teacher(dummy_input)\n",
    "print(output.shape)  # Должно быть torch.Size([1, 2])\n",
    "print(\"Выход учителя:\", teacher(dummy_input).shape)  # torch.Size([1, 3])\n",
    "\n",
    "# Для студента\n",
    "print(\"Выход студента:\", student(dummy_input))\n",
    "\n",
    "student = DeiT(\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    emb_size=384,\n",
    "    img_size=224,\n",
    "    depth=12,\n",
    "    n_classes=3  # Два класса\n",
    ")\n",
    "\n",
    "# Оптимизатор\n",
    "optimizer = Adam(student.parameters(), lr=0.001)\n",
    "\n",
    "# Функция потерь\n",
    "criterion = HardDistillationLoss(teacher)\n",
    "\n",
    "# Цикл обучения\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "try:\n",
    "    for epoch in range(10):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dl, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = student(inputs)  # Должен вернуть (outputs_cls, outputs_dist)\n",
    "            loss = criterion(inputs, outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dl):.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"Ошибка:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e15ebb91-1990-49d9-a9c7-bb23c5452130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.5891, -0.1246],\n",
      "        [-0.5992, -0.1299]], grad_fn=<AddmmBackward0>), tensor([[ 3.9488e-07,  6.1619e-01],\n",
      "        [-2.4278e-03,  6.2632e-01]], grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(2, 3, 224, 224)  # Batch size = 2\n",
    "output = student(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d71397c-f190-469c-ae19-7462103e1150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: {1}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for _, labels in dl:\n",
    "    unique_labels.update(labels.tolist())\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "224b6881-1616-4b7c-9f1e-7287c42ba85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метки в батче: tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for _, labels in dl:\n",
    "    print(\"Метки в батче:\", labels.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28094bb-da00-4251-ba67-a7db50b30992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
