{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac62347e-93d4-4c7b-99df-f88439f5c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import timm\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4355b566-2bf0-4e29-9090-0ee6fe43f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#определение DeiT модели\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 384, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e h w -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positions = nn.Parameter(torch.randn(num_patches + 2, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        dist_tokens = repeat(self.dist_token, '() n e -> b n e', b=b)\n",
    "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7080c2d6-f1f1-4028-aafe-d0129234cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size: int = 384, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.head = nn.Linear(emb_size, n_classes)\n",
    "        self.dist_head = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x_cls, x_dist = x[:, 0], x[:, 1]\n",
    "        x_head = self.head(x_cls)\n",
    "        x_dist_head = self.dist_head(x_dist)\n",
    "        if self.training:\n",
    "            return x_head, x_dist_head\n",
    "        else:\n",
    "            return (x_head + x_dist_head) / 2\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 384, num_heads: int = 6, dropout: float = 0.):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 384, num_heads: int = 6, drop_p: float = 0., forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[\n",
    "            TransformerEncoderBlock(**kwargs)\n",
    "            for _ in range(depth)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979e7597-5e6c-427d-8902-a7585cfda512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Sequential):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 384,\n",
    "                 img_size: int = 224, depth: int = 12, n_classes: int = 2):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f116fa4-6b28-42e9-a4d8-2758a4edf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#определение Loss и Grad-CAM\n",
    "class HardDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs: Tensor, outputs: tuple[Tensor, Tensor], labels: Tensor) -> Tensor:\n",
    "        outputs_cls, outputs_dist = outputs\n",
    "        base_loss = self.criterion(outputs_cls, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(inputs)\n",
    "        teacher_logits = teacher_outputs[:, :2]\n",
    "        teacher_labels = torch.argmax(teacher_logits, dim=1)\n",
    "\n",
    "        teacher_loss = self.criterion(outputs_dist, teacher_labels)\n",
    "\n",
    "        return 0.5 * base_loss + 0.5 * teacher_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aebed2b-e228-40de-bce0-ec82278b60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        target_layer.register_forward_hook(self.save_activations)\n",
    "        target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def forward(self, x, class_idx=None):\n",
    "        original_size = x.shape[-2:]\n",
    "        h, w = original_size\n",
    "        logits = self.model(x)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits[0]\n",
    "        self.model.zero_grad()\n",
    "        if class_idx is None:\n",
    "            class_idx = logits.argmax(dim=1).item()\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot[0][class_idx] = 1\n",
    "        one_hot.requires_grad_(True)\n",
    "        output = (one_hot * logits).sum()\n",
    "        output.backward(retain_graph=True)\n",
    "        gradients = self.gradients.cpu().numpy()[0]\n",
    "        activations = self.activations.cpu().numpy()[0]\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (int(w), int(h)))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        return self.forward(x, class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7b297-4580-478d-b15b-3bcc16e71638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████████████████████████| 193/193 [44:37<00:00, 13.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.8754, Acc: 0.5351, F1: 0.4813, AUC: 0.5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████| 193/193 [41:38<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Loss: 0.7089, Acc: 0.5668, F1: 0.5091, AUC: 0.5808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████| 193/193 [47:31<00:00, 14.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Loss: 0.7032, Acc: 0.5723, F1: 0.5180, AUC: 0.5953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  26%|████████                       | 50/193 [14:02<39:42, 16.66s/it]"
     ]
    }
   ],
   "source": [
    "# подготовка данных и моделей\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = datasets.ImageFolder(root='Testing', transform=transform)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=2)\n",
    "student = DeiT(n_classes=2)\n",
    "\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
    "criterion = HardDistillationLoss(teacher)\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# 🏋️‍♂️ 4. Обучение\n",
    "# ---------------------------\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_f1_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(5):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "        for batch in tqdm(dl, desc=f\"Epoch {epoch+1}/5\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = student(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs_cls, outputs_dist = outputs\n",
    "            else:\n",
    "                outputs_cls = outputs\n",
    "                outputs_dist = outputs\n",
    "            loss = criterion(inputs, (outputs_cls, outputs_dist), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs_cls, dim=1).detach().cpu().numpy()\n",
    "            preds = probs.argmax(axis=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs[:, 1])\n",
    "\n",
    "        epoch_loss = running_loss / len(dl)\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "        epoch_f1 = f1_score(all_labels, all_preds)\n",
    "        epoch_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        train_f1_scores.append(epoch_f1)\n",
    "        train_auc_scores.append(epoch_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/5 | Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, F1: {epoch_f1:.4f}, AUC: {epoch_auc:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Ошибка:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c600a4-7c44-4957-bbf2-35c960a7dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 🤖 5. Метамодель уверенности\n",
    "# ---------------------------\n",
    "\n",
    "X_meta = []\n",
    "y_meta = []\n",
    "\n",
    "student.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dl:\n",
    "        images = images.to(device)\n",
    "        outputs = student(images)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        true_labels = labels.numpy()\n",
    "\n",
    "        for i in range(len(true_labels)):\n",
    "            p0, p1 = probs[i]\n",
    "            max_p = max(p0, p1)\n",
    "            entr = entropy([p0, p1])\n",
    "            X_meta.append([p0, p1, max_p, entr])\n",
    "            y_meta.append(1 if preds[i] == true_labels[i] else 0)\n",
    "\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "meta_model.fit(X_meta, y_meta)\n",
    "joblib.dump(meta_model, \"meta_model.pkl\")\n",
    "print(\"Метамодель сохранена как meta_model.pkl\")\n",
    "\n",
    "\n",
    "def classify(image_tensor):\n",
    "    student.eval()\n",
    "    with torch.no_grad():\n",
    "        output = student(image_tensor.unsqueeze(0).to(device))\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    p0, p1 = probs\n",
    "    entr = entropy([p0, p1])\n",
    "    max_p = max(p0, p1)\n",
    "    features = [[p0, p1, max_p, entr]]\n",
    "    trust = meta_model.predict(features)[0]\n",
    "\n",
    "    if trust == 1:\n",
    "        return \"здоров\" if p0 > p1 else \"болен\"\n",
    "    else:\n",
    "        return \"аномальное\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8389b-395a-43a9-80ff-4bf2023afe4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf369b4-10ea-48d5-945f-d13e572ae9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = ds[0]\n",
    "result = classify(image)\n",
    "print(\"Результат классификации:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a68329-b1c9-417d-bf18-d916846c6320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
