{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e8166e-c0f4-4244-bcbe-aa81f6ba2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brain Tumor', 'Healthy']\n",
      "6161\n",
      "Класс: Healthy, вероятность: 1.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "from einops.layers.torch import Rearrange\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class HardDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs: Tensor, outputs: tuple[Tensor, Tensor], labels: Tensor) -> Tensor:\n",
    "        outputs_cls, outputs_dist = outputs\n",
    "\n",
    "        # Базовая потеря (CLS)\n",
    "        base_loss = self.criterion(outputs_cls, labels)\n",
    "\n",
    "        # Вычисляем предсказания учителя\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(inputs)\n",
    "\n",
    "        # Ограничиваем выходы учителя двумя классами\n",
    "        teacher_logits = teacher_outputs[:, :2]  # Берем только первые два класса\n",
    "        teacher_labels = torch.argmax(teacher_logits, dim=1)\n",
    "\n",
    "        # Потеря для DIST\n",
    "        teacher_loss = self.criterion(outputs_dist, teacher_labels)\n",
    "\n",
    "        # Комбинируем потери\n",
    "        return 0.5 * base_loss + 0.5 * teacher_loss\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 384, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Проекция патчей\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "        # Токены CLS и DIST\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, emb_size))  # Убедитесь, что это определено\n",
    "\n",
    "        # Позиционные эмбеддинги\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positions = nn.Parameter(torch.randn(num_patches + 2, emb_size))  # +2 для cls_token и dist_token\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "\n",
    "        # Проекция патчей\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Создание токенов CLS и DIST\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        dist_tokens = repeat(self.dist_token, '() n e -> b n e', b=b)\n",
    "\n",
    "        # Добавление токенов CLS и DIST к входным данным\n",
    "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)\n",
    "\n",
    "        # Добавление позиционных эмбеддингов\n",
    "        x += self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 2):       \n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Linear(emb_size, n_classes)\n",
    "        self.dist_head = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, x_dist = x[:, 0], x[:, 1]\n",
    "        x_head = self.head(x)\n",
    "        x_dist_head = self.dist_head(x_dist)\n",
    "        \n",
    "        if self.training:\n",
    "            x = x_head, x_dist_head  # Возвращает кортеж\n",
    "        else:\n",
    "            x = (x_head + x_dist_head) / 2  # Возвращает тензор\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy = energy.masked_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.emb_size ** 0.5\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhqk, bhkd -> bhqd', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class DeiT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes))\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Изменяем размер до 224x224\n",
    "    transforms.ToTensor(),          # Преобразуем в тензор\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация\n",
    "])\n",
    "\n",
    "# Создание датасета с помощью ImageFolder\n",
    "ds = datasets.ImageFolder(root='Testing', transform=transform)\n",
    "\n",
    "# Создание DataLoader\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=False)\n",
    "\n",
    "print(ds.classes)  # ['tumor', 'no_tumor']\n",
    "print(len(ds))\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Hook для сохранения градиентов и активаций\n",
    "        target_layer.register_forward_hook(self.save_activations)\n",
    "        target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def forward(self, x, class_idx=None):\n",
    "        # Сохраняем исходные размеры изображения\n",
    "        original_size = x.shape[-2:]  # (height, width)\n",
    "        h, w = original_size\n",
    "        print('h: ', h, 'w: ', w)\n",
    "    \n",
    "        # Проверка, что размеры корректны\n",
    "        if h <= 0 or w <= 0:\n",
    "            raise ValueError(f\"Некорректные размеры изображения: height={h}, width={w}\")\n",
    "    \n",
    "        # Прямой проход через модель\n",
    "        logits = self.model(x)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits[0]  # Берём первый выход (CLS)\n",
    "        self.model.zero_grad()\n",
    "    \n",
    "        if class_idx is None:\n",
    "            class_idx = logits.argmax(dim=1).item()\n",
    "    \n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot[0][class_idx] = 1\n",
    "        one_hot.requires_grad_(True)\n",
    "        \n",
    "        # Вычисляем градиенты относительно one_hot\n",
    "        output = (one_hot * logits).sum()\n",
    "        output.backward(retain_graph=True)\n",
    "    \n",
    "        gradients = self.gradients.cpu().numpy()[0]\n",
    "        activations = self.activations.cpu().numpy()[0]\n",
    "    \n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        print('h: ', h, 'w: ', w)\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "    \n",
    "        for i, w in enumerate(weights):\n",
    "            w = 224\n",
    "            cam += w * activations[i]\n",
    "    \n",
    "        cam = np.maximum(cam, 0)\n",
    "        print('h: ', h, 'w: ', w)\n",
    "    \n",
    "        # Проверка размеров перед изменением размера\n",
    "        if int(w) <= 0 or int(h) <= 0:\n",
    "            raise ValueError(f\"Некорректные размеры для изменения размера: w={w}, h={h}\")\n",
    "        # w=224    \n",
    "        cam = cv2.resize(cam, (int(w), int(h)))  # Преобразуем w и h в целые числа\n",
    "        \n",
    "    \n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        return self.forward(x, class_idx)\n",
    "\n",
    "\n",
    "def load_model_for_analysis(model_path: str, device: str = 'cpu'):\n",
    "    \"\"\"\n",
    "    Загружает сохранённую модель PyTorch для анализа.\n",
    "    :param model_path: путь к .pth файлу\n",
    "    :param device: 'cpu' или 'cuda' (по умолчанию 'cpu')\n",
    "    :return: модель\n",
    "    \"\"\"\n",
    "    model = DeiT(\n",
    "        in_channels=3,\n",
    "        patch_size=16,\n",
    "        emb_size=384,\n",
    "        img_size=224,\n",
    "        depth=12,\n",
    "        n_classes=2\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu', weights_only=False))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Пример использования:\n",
    "# model = load_model_for_analysis('DeiT.pth', device='cpu')\n",
    "\n",
    "def predict_image(model, image_path, transform, class_names, device='cpu'):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Если модель возвращает кортеж (как DeiT с distillation),\n",
    "        # используем среднее между CLS и DIST головами\n",
    "        if isinstance(output, tuple):\n",
    "            cls_output, dist_output = output\n",
    "            probs = torch.softmax((cls_output + dist_output) / 2, dim=1)\n",
    "        else:\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "\n",
    "        pred_idx = probs.argmax(dim=1).item()\n",
    "        pred_class = class_names[pred_idx]\n",
    "        pred_prob = probs[0, pred_idx].item()\n",
    "\n",
    "    return pred_class, pred_prob\n",
    "\n",
    "# Пример использования:\n",
    "# model = load_model_for_analysis('/Users/ilia/DeiT/DeiT.pth')\n",
    "# class_name, prob = predict_image(model, 'path/to/image.jpg', transform, ds.classes)\n",
    "# print(f'Класс: {class_name}, вероятность: {prob:.2f}')\n",
    "\n",
    "# 1. Загружаем модель\n",
    "model = load_model_for_analysis('/Users/ilia/DeiT/DeiT_weights.pth', device='cpu')\n",
    "\n",
    "# 2. Делаем предсказание\n",
    "class_name, prob = predict_image(model, '/Users/ilia/DataSetForDiplom/archive-6/Testing/Health/Te-pi_0290.jpg', transform, ds.classes)\n",
    "print(f'Класс: {class_name}, вероятность: {prob:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcff564-e0f2-4810-b496-3e21d9f91922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
